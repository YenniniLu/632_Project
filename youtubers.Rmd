---
title: "632 Project"
output:
  html_document:
    df_print: paged
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning=FALSE)
```

```{r message=FALSE}
library(tidyverse)
library(stringr)
library(randomForest) 
library(vip) 
library(rpart)
library(rpart.plot)
library(caret)
library(tidytext)
library(tidyr)
library(MASS)
library(car)
```

## Research Data

The dataset contains **2515 unique videos** and their **subtitles** from over **91 different YouTubers**, ranging from all different kinds of categories. 

```{r}
df_raw <- read.csv("data.csv")
head(df_raw)
```

![](assets/video.png)

## Research Goal

Do a sentiment analysis on the subtitles and find the best multiple linear regression model to predict the number of views using Subscribers, CC, Released, Category, Sentiment and Length.

## Tasks

1. Data Cleaning.       
2. Conduct a sentiment analysis on the subtitles.         
3. Try various statistical models like linear regression, decision tree and random forest.        
4. Compare these models in terms of prediction performation and interpretability.       

## Data Cleaning


```{r}
df_raw %>% select(Views, Subscribers, Released, Length) %>% head(10)
```

Looking at the data, we notice several problems in the data, like:     

1. Views: The Views variable is in string format and the units are different, like "10K views", "10M views". We prefer it to be in number and in the same unit in order to conduct statistical analysis.       

2. Subscribers: The same problems as Views. The Subscribers variable is like "10K subscribers", "10M subscribers".       

3. Length: The video length is in string format, like "12:00", "1:12:00". We need it to be in number and in the same unit.       

4. Released: The Released variable is in string format, like "2 years age", "10 month ago". We need it to be in number and in the same unit.        

Therefore, we need to do data cleaning first.

```{r eval=FALSE}
# Unify units and convert string to number, like: 10K views -> 10, 10M views -> 10000
cleanViews <- function(str) {
  str <- str_remove(str, " views")
  last <- str_sub(str, -1)
  views <- str %>% str_remove(last) %>% as.numeric()
  if (last == "M") return(1000*views)
  else return(views)
}

# Unify units and convert string to number, like: 10K subscribers -> 10, 10M subscribers -> 10000
cleanSubscribers <- function(str) {
  str <- str_remove(str, " subscribers")
  last <- str_sub(str, -1)
  views <- str %>% str_remove(last) %>% as.numeric()
  if (last == "M") return(1000*views)
  else return(views)
}

# Convert time in string format to number of minutes, like: 12:00 -> 12, 1:12:00 -> 72
cleanLength <- function(str) {
  list <- str_split(str, ":")
  len <- length(list[[1]])
  if (len == 3) {
     h <- as.numeric(list[[1]][1])
     m <- as.numeric(list[[1]][2])
     return((m + 1) + 60*h)
  } else {
     m <- as.numeric(list[[1]][1])
     return(m+1)
  }
}

# Convert time to number of months ago, like: 1 years ago -> 12, 10 months ago to 10
cleanReleased <- function(str) {
  str <- str_remove(str, "Streamed ")
  list <- str_split(str, " ")
  if (list[[1]][2] == "years") return(as.numeric(list[[1]][1])*12)
  else return(as.numeric(list[[1]][1]))
}

# Remove NAs
df <- df_raw %>%  
  filter(
    !is.na(Released) & Released != ""
  ) 

# Clean the data
df <- df %>% mutate(
  Views = map_dbl(Views, cleanViews),
  Subscribers = map_dbl(Subscribers, cleanSubscribers),
  Length = map_dbl(Length, cleanLength),
  Released = map_dbl(Released, cleanReleased)
)

df %>% select(Views, Subscribers, Released, Length) %>% head(10)

# Save for future use
write_csv(df, "cleaned_data.csv")
```
After cleaning, Views, Subscribers, Released and Length are numbers, while Views, Subscribers are in K and Released and Length are in minute.

## Data Discovery / Diagnostics for Linear Regression

```{r}
df <- read_csv("cleaned_data.csv")
df$CC <- as.factor(df$CC)
df$Category <- as.factor(df$Category)
df$Subscribers <- as.numeric(df$Subscribers)

head(df)
```

```{r}
table(df$CC)
boxplot(log(Views) ~ CC, data = df)

hist(df$Length, xlab = "Minutes", breaks = 50)
summary(df$Length) 

summary(df$Released)
hist(df$Released, xlab = "Month Ago")

summary(df$Subscribers)
hist(df$Subscribers, xlab = "K Subscribers")

pairs(Views ~ CC + Released + Length + Subscribers + Category, data=df)
```


```{r}
hist(df$Views)
```
From the diagnostics, Views, Subscribers, Released and Length need log transformation.


## Sentiment Analysis / Text mining

TODO by Xinyi
```{r}
df_script <- df %>% 
  select(Title, Transcript)
head(df_script)
```

```{r}
# just use this code to watch the video to check the transcript
df %>% 
  filter(Title == "Former CIA Agent Breaks Down Jeffrey Epstein Case")
```


```{r}
data("stop_words")
custom_stop_words <- rbind(stop_words, c("_", "custom"))
```

```{r}
#bigram
bigrams_separated <- df_script %>% 
  group_by(Title) %>% 
  # unnest Transcript in bigram format
  unnest_tokens(bigram, Transcript, token = "ngrams", n = 2) %>% 
  separate(bigram, c("word1", "word2"), sep = " ") %>% # separate bigram 
  filter(!word1 %in% custom_stop_words$word) %>% # filter out all the stop words
  filter(!word2 %in% custom_stop_words$word) 
  
bigrams_united <- bigrams_separated %>% 
  unite(bigram, word1, word2, sep = " ")  # unite words back together

```

```{r}
head(bigrams_separated)
```



```{r}
# bigram tf-idf
bigram_tf_idf <- bigrams_united %>%
  count(Title, bigram) %>%
  bind_tf_idf(bigram, Title, n) %>%
  arrange(desc(tf_idf))

head(bigram_tf_idf)
```



```{r}
# Using bigrams to provide context in sentiment analysis
# not in presentation!!
negation_words <- c("not", "no", "never", "without")

bigrams_separated %>%
  filter(word1 %in% negation_words) %>%
  inner_join(get_sentiments("afinn"), by = c(word2 = "word")) %>%
  count(word1, word2, value, sort = TRUE)

```


```{r}
df_word <- df_script %>% 
  group_by(Title) %>% 
  unnest_tokens(word, Transcript) %>% 
  anti_join(custom_stop_words) %>% 
  count(word, sort = TRUE) %>% 
  mutate(total = sum(n)) %>% 
  ungroup()
  #inner_join(get_sentiments("afinn")) #%>% 
  #summarise(sentiment = sum(value))
  #mutate(total = sum(word)) %>% 
  #mutate(perc = round(n/total, 2))

head(df_word)
```

```{r}
df_title_word <- df %>% 
  group_by(Id) %>% 
  unnest_tokens(word, Title) %>% 
  anti_join(custom_stop_words) %>% 
  count(word, sort = TRUE) %>% 
  mutate(total = sum(n)) %>% 
  ungroup()
  #inner_join(get_sentiments("afinn")) #%>% 
  #summarise(sentiment = sum(value))
  #mutate(total = sum(word)) %>% 
  #mutate(perc = round(n/total, 2))

head(df_title_word)
```





Below codes:
use tf-idf to find the importance of a word in the transcript, then times it to a word's afinn score, and sum up all the words' socre to a video afinn_score.
Will use "afinn_score" to represent the sentiment score in the regression modeling!

Transcript sentiment
```{r}
df_afinn <- df_word %>% 
  inner_join(get_sentiments("afinn")) %>% 
  #mutate(afinn_score = sum(value)) %>% 
  #mutate(perc = round(n/total, 2)) %>% 
  group_by(Title) %>% 
  bind_tf_idf(word, Title, n) %>% 
  mutate(afinn_score = sum(value*tf_idf)) %>%
  ungroup() 
  #filter(Title == "2018 Jeep Trackhawk Review - The SUV That's Quicker Than a Supercar")

head(df_afinn)
```

```{r}
df_afinn <- df_afinn %>% 
  select(Title, afinn_score) %>% 
  unique() %>% 
  ungroup()

df_afinn
```


```{r}
summary(df_afinn$afinn_score)
```

```{r}
hist(df_afinn$afinn_score)
```

Title sentiment
```{r}
df_title_afinn <- df_title_word %>% 
  inner_join(get_sentiments("afinn")) %>% 
  #mutate(afinn_score = sum(value)) %>% 
  #mutate(perc = round(n/total, 2)) %>% 
  group_by(Id) %>% 
  bind_tf_idf(word, Id, n) %>% 
  mutate(afinn_title_score = sum(value*tf_idf)) %>%
  ungroup() 
  #filter(Title == "2018 Jeep Trackhawk Review - The SUV That's Quicker Than a Supercar")

head(df_title_afinn)
```


```{r}
df_title_afinn <- df_title_afinn %>% dplyr::select(Id, afinn_title_score) %>% 
  unique() %>% 
  ungroup()

head(df_title_afinn)
```

```{r}
summary(df_title_afinn$afinn_title_score)
```

```{r}
hist(df_title_afinn$afinn_title_score)
```



```{r}
df1 <- df %>% 
  right_join(df_afinn) %>% 
  right_join(df_title_afinn) %>% 
  unique() 


head(df1)
```


## Preparation for Cross-Validation

Randomly split the data set in a 70% training and 30% test set. Make sure to use set.seed() so that your results are reproducible
 
```{r}
set.seed(652)
n <- nrow(df1)
train_index <- sample(1:n, round(0.7*n))
df_train <- df[train_index,]
df_test <- df[-train_index,]
```

## Linear Regression

```{r}
pairs(log10(Views) ~ CC + log10(Released) + 
        log10(Length) + log10(Subscribers) + Category + 
        afinn_score + afinn_title_score, 
      data=df1)
```


```{r}
plot(log10(Views)~ afinn_score, data = df1)
```


```{r}
plot(log10(Views)~ afinn_title_score, data = df1)
```


```{r}
boxcox(Views ~ CC + log10(Released) + log10(Length) + log10(Subscribers) + Category + afinn_score + afinn_title_score,
       data=df1,
       lambda = seq(-3, 3, by = 0.05))
```

```{r}
summary(powerTransform(Views ~ 
                         CC + log10(Released) + log10(Length) +
                         log10(Subscribers) + Category + afinn_score + afinn_title_score, 
                       data=df1))
```


```{r}
lm1 <- lm(log10(Views) ~ CC + log10(Released) + log10(Length) +
            log10(Subscribers) + Category + afinn_score+ afinn_title_score, 
          data=df1)
summary(lm1)
```

```{r}
plot(predict(lm1), resid(lm1), xlab = "Fitted values", ylab = "Residuals")
abline(h=0)
```

```{r}
par(mfrow=c(1,2))
hist(resid(lm1))
qqnorm(resid(lm1))
qqline(resid(lm1))
```



```{r}
lm2 <- lm(log10(Views) ~ CC + log10(Length) + log10(Subscribers) + Category + afinn_score, data=df1)
summary(lm2)
```

```{r}
plot(predict(lm2), resid(lm2), xlab = "Fitted values", ylab = "Residuals")
abline(h=0)
```


```{r}
par(mfrow=c(1,2))
hist(resid(lm2))
qqnorm(resid(lm2))
qqline(resid(lm2))
```


```{r}
lm3 <- lm(log10(Views) ~ CC  + log10(Length) + log10(Subscribers) + afinn_score, data=df1)
summary(lm3)
```

```{r}
plot(predict(lm3), resid(lm3), xlab = "Fitted values", ylab = "Residuals")
abline(h=0)
```
```{r}
par(mfrow=c(1,2))
hist(resid(lm3))
qqnorm(resid(lm3))
qqline(resid(lm3))
```


```{r}
lm4 <- step(lm1)
summary(lm4)
```


## Regression Tree

### Fit a regression tree on the training set.

```{r}
# Fit tree model
t1 <- rpart(Views ~ CC + Released + Category + Length + Subscribers,
            data = df_train,
            method = "anova")

# Plot the desicion tree
rpart.plot(t1)

# Plot R-square vs Splits and the Relative Error vs Splits.
rsq.rpart(t1)
```

### Make predictions on the test set and compute the RMSE

```{r}
# Make prediction
pred_tree <- predict(t1, newdata = df_test)

# Compute the RMSE
RMSE(df_test$Views, pred_tree)
```

## Random Forest

### Fit a Random Forest on the training set usinng the defaults for mtry and ntree.

```{r}
set.seed(652)
rf1 <- randomForest(Views ~ CC + Released + Category + Length + Subscribers, importance = TRUE, data = df_train)
rf1
```

Use the `vip()` function to make a variable importance plot.  Which variables are most important? 

```{r}
vip(rf1, num_features = 14,  include_type = TRUE)
```

### Make predictions on the test set and compute the RMSE

```{r}
# Make prediction
pred_rf <- predict(rf1, newdata = df_test)

# Compute the RMSE
RMSE(df_test$Views, pred_rf)
```

## Conclusion(TODO)

Model | RMSE | R Squared | Number of Coefficients | performance | interpretability
---- | ---- | ---- | ---- | ---- | ----
linear regression | - | - | - | - | -
regression tree | - | - | - | - | -
random forest | - | - | - | - | -

### Linear Regression vs Regression Tree VS Random Forest 

Aggregated/ensemble models are not universally better than their "single" counterparts, they are better if and only if the single models suffer of instability. With XX training rows and only XX columns, we are in a comfortable training sample size situation in which even a decision tree may get reasonably stable.



